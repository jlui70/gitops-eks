# Conceitos Fundamentais - Kubernetes no EKS

## ğŸ“š Guia Educacional para Professores e Alunos

Este documento explica os conceitos fundamentais observados no cluster EKS criado por este projeto, ideal para uso em aulas e treinamentos.

---

## ğŸ–¥ï¸ NODES (Worker Nodes)

### Por que 3 Nodes?

**Conceito**: Nodes sÃ£o as mÃ¡quinas (EC2 instances) que executam os containers/pods.

**Por que 3?**
- âœ… **Alta Disponibilidade (HA)**: Se 1 node falhar, os outros 2 continuam funcionando
- âœ… **DistribuiÃ§Ã£o de Carga**: Workloads sÃ£o distribuÃ­dos entre os 3 nodes
- âœ… **Best Practice**: NÃºmero Ã­mpar previne "split-brain" em decisÃµes de cluster
- âœ… **Zonas de Disponibilidade**: Cada node em uma AZ diferente (us-east-1a, 1b, 1c)

**Analogia**: Como ter 3 servidores fÃ­sicos em um data center, se 1 cair, os outros 2 mantÃªm o serviÃ§o.

**VerificaÃ§Ã£o no projeto**:
```bash
kubectl get nodes
# Mostra: ip-10-0-1-110, ip-10-0-1-15, ip-10-0-1-68
# Cada um em uma subnet/AZ diferente
```

---

## ğŸ“¦ PODS

### O que Ã© um Pod?

**Conceito**: Menor unidade no Kubernetes. Um "envelope" que contÃ©m 1 ou mais containers.

### Por que mÃºltiplos Pods do mesmo tipo?

#### 1. **Deployments (2-3 rÃ©plicas)** - Para **Alta Disponibilidade**

| Pod | RÃ©plicas | Motivo |
|-----|----------|--------|
| `aws-load-balancer-controller` | 2 | Se 1 cair, o outro assume |
| `coredns` | 2 | DNS sempre disponÃ­vel |
| `ebs-csi-controller` | 2 | Gerenciamento de volumes resiliente |
| `metrics-server` | 2 | Monitoramento contÃ­nuo |

**Analogia**: Como ter 2 atendentes no caixa - se 1 tirar pausa, o outro continua atendendo.

#### 2. **DaemonSets (1 por node = 3 total)** - Para **Cobertura em Todos os Nodes**

| Pod | Quantidade | Motivo |
|-----|------------|--------|
| `aws-node` | 3 (1/node) | Networking em cada node |
| `kube-proxy` | 3 (1/node) | Roteamento em cada node |
| `ebs-csi-node` | 3 (1/node) | Acesso a volumes EBS em cada node |

**Analogia**: Como ter 1 seguranÃ§a em cada andar de um prÃ©dio - cada andar precisa de cobertura.

---

## ğŸ”„ REPLICASETS

### O que sÃ£o?

**Conceito**: Mecanismo que **garante que X rÃ©plicas de um pod estejam sempre rodando**.

### RelaÃ§Ã£o: Deployment â†’ ReplicaSet â†’ Pods

```
Deployment (desejado: 2 rÃ©plicas)
    â†“
ReplicaSet (gerencia: 2 pods)
    â†“
Pods (executa: 2 containers)
```

### Exemplo PrÃ¡tico:

```yaml
# VocÃª define no Deployment:
replicas: 2

# Kubernetes cria automaticamente:
ReplicaSet â†’ cria 2 pods
             â†“
Se 1 pod morrer â†’ ReplicaSet cria outro automaticamente
```

### Os 4 ReplicaSets no projeto:

1. **aws-load-balancer-controller**: MantÃ©m 2 pods gerenciando ALBs
2. **coredns**: MantÃ©m 2 pods fazendo DNS resolution
3. **ebs-csi-controller**: MantÃ©m 2 pods gerenciando volumes EBS
4. **metrics-server**: MantÃ©m 2 pods coletando mÃ©tricas

**VerificaÃ§Ã£o**:
```bash
kubectl get replicasets -n kube-system
# Mostra: DESIRED 2, CURRENT 2, READY 2
```

---

## ğŸš€ DEPLOYMENTS

### O que sÃ£o?

**Conceito**: Forma **declarativa** de gerenciar aplicaÃ§Ãµes. VocÃª diz "quero 2 rÃ©plicas" e o Kubernetes garante isso.

### Os 4 Deployments no Console AWS:

#### 1. **aws-load-balancer-controller** (2 rÃ©plicas)
- **FunÃ§Ã£o**: Gerencia Application Load Balancers (ALBs) da AWS
- **Por quÃª?**: Quando vocÃª cria um Ingress, este controller provisiona um ALB automaticamente
- **Sem ele**: VocÃª teria que criar ALBs manualmente na AWS Console
- **Arquivos relacionados**: `02-eks-cluster/eks.cluster.external.alb.tf`

#### 2. **coredns** (2 rÃ©plicas)
- **FunÃ§Ã£o**: DNS interno do cluster
- **Por quÃª?**: Resolve nomes como `service-name.namespace.svc.cluster.local`
- **Exemplo**: Pod chama `product-catalog` â†’ CoreDNS resolve para IP do serviÃ§o
- **Sem ele**: Pods nÃ£o conseguem se comunicar por nome

#### 3. **ebs-csi-controller** (2 rÃ©plicas)
- **FunÃ§Ã£o**: Controlador para volumes EBS (storage)
- **Por quÃª?**: Permite criar/attach/detach volumes EBS dinamicamente
- **Exemplo**: Banco de dados MongoDB precisa de disco persistente
- **Arquivos relacionados**: `02-eks-cluster/eks.cluster.addons.csi.tf`

#### 4. **metrics-server** (2 rÃ©plicas)
- **FunÃ§Ã£o**: Coleta mÃ©tricas de CPU/MemÃ³ria dos pods
- **Por quÃª?**: Habilita `kubectl top pods` e Horizontal Pod Autoscaling (HPA)
- **Comando**: `kubectl top nodes` / `kubectl top pods`
- **Arquivos relacionados**: `02-eks-cluster/eks.cluster.addons.metrics-server.tf`

### Diagrama do Fluxo:

```
VocÃª â†’ kubectl apply deployment.yaml
         â†“
    Deployment (define: 2 rÃ©plicas)
         â†“
    ReplicaSet (cria: 2 pods)
         â†“
    Pods (executam: containers)
         â†“
    Nodes (hospedam: pods)
```

---

## ğŸ‘¹ DAEMONSETS

### O que sÃ£o?

**Conceito**: Garante que **1 pod rode em CADA node** do cluster.

**DiferenÃ§a para Deployments**:
- **Deployment**: "Quero 2 rÃ©plicas no cluster" (Kubernetes escolhe onde)
- **DaemonSet**: "Quero 1 rÃ©plica em CADA node" (obrigatÃ³rio em todos)

### Os 4 DaemonSets no projeto:

#### 1. **aws-node** (3 pods - 1 por node)
- **FunÃ§Ã£o**: Plugin CNI (Container Network Interface) da AWS VPC
- **Por quÃª?**: Atribui IPs da VPC aos pods
- **Como funciona**: Cada pod no node recebe um IP do range da subnet
- **Sem ele**: Pods nÃ£o teriam conectividade de rede
- **Add-on relacionado**: Amazon VPC CNI

#### 2. **kube-proxy** (3 pods - 1 por node)
- **FunÃ§Ã£o**: Gerencia regras de rede (iptables/IPVS) para Services
- **Por quÃª?**: Permite comunicaÃ§Ã£o entre pods atravÃ©s de Services
- **Exemplo**: Request para `http://product-catalog:8080` â†’ kube-proxy roteia para pod correto
- **Sem ele**: Services nÃ£o funcionariam

#### 3. **ebs-csi-node** (3 pods - 1 por node)
- **FunÃ§Ã£o**: Agent que monta volumes EBS nos nodes
- **Por quÃª?**: Trabalha com ebs-csi-controller para attach volumes
- **Exemplo**: Pod com PVC â†’ este agent monta o EBS no node â†’ pod acessa disco
- **Add-on relacionado**: Amazon EBS CSI Driver

#### 4. **ebs-csi-node-windows** (0 pods - porque nÃ£o temos nodes Windows)
- **FunÃ§Ã£o**: Mesma do ebs-csi-node, mas para Windows nodes
- **Por quÃª?**: Opcional, sÃ³ roda se vocÃª tiver nodes Windows no cluster
- **Status**: 0 pods porque este projeto usa apenas Linux nodes

### Diagrama DaemonSet:

```
Cluster com 3 nodes:
    
Node 1 (us-east-1a)          Node 2 (us-east-1b)          Node 3 (us-east-1c)
â”œâ”€â”€ aws-node                 â”œâ”€â”€ aws-node                 â”œâ”€â”€ aws-node
â”œâ”€â”€ kube-proxy               â”œâ”€â”€ kube-proxy               â”œâ”€â”€ kube-proxy
â”œâ”€â”€ ebs-csi-node             â”œâ”€â”€ ebs-csi-node             â”œâ”€â”€ ebs-csi-node
â””â”€â”€ [seus apps]              â””â”€â”€ [seus apps]              â””â”€â”€ [seus apps]

DaemonSet garante: 1 pod em CADA node
```

### Caso de Uso - Adicionando um 4Âº Node:

```bash
# Se vocÃª adicionar um 4Âº node ao cluster:
# DaemonSets automaticamente criam pods no novo node:
Node 4 (us-east-1a)
â”œâ”€â”€ aws-node      â† criado automaticamente
â”œâ”€â”€ kube-proxy    â† criado automaticamente
â””â”€â”€ ebs-csi-node  â† criado automaticamente
```

---

## ğŸ§© ADD-ONS

### O que sÃ£o?

**Conceito**: ExtensÃµes **opcionais** que adicionam funcionalidades ao cluster EKS.

**DiferenÃ§a**: 
- **Core Kubernetes**: kubectl, API server, scheduler (vem por padrÃ£o)
- **Add-ons**: Funcionalidades extras instaladas separadamente

### Os 3 Add-ons no projeto:

#### 1. **Amazon VPC CNI** 
```
Categoria: Networking
FunÃ§Ã£o: IntegraÃ§Ã£o de rede entre pods e VPC AWS
```

**O que faz**:
- Atribui IPs da VPC diretamente aos pods
- Permite pods se comunicarem com recursos AWS (RDS, S3, etc) nativamente
- Implementa Security Groups para pods

**Por quÃª Ã© necessÃ¡rio**:
- EKS roda na AWS VPC
- Pods precisam IPs vÃ¡lidos na VPC para se comunicar
- Alternativas (Calico, Flannel) nÃ£o tÃªm integraÃ§Ã£o nativa com AWS

**Componente relacionado**: DaemonSet `aws-node`

**Arquivo Terraform**: `02-eks-cluster/eks.cluster.addons.csi.tf`

---

#### 2. **Amazon EBS CSI Driver**
```
Categoria: Storage
FunÃ§Ã£o: Gerenciamento de volumes persistentes (EBS)
```

**O que faz**:
- Cria/attach/detach volumes EBS dinamicamente
- Permite usar StorageClasses para provisionamento automÃ¡tico
- Suporta snapshots e resize de volumes

**Por quÃª Ã© necessÃ¡rio**:
- AplicaÃ§Ãµes stateful (bancos de dados) precisam de storage persistente
- Volumes devem sobreviver se o pod morrer
- EBS Ã© o storage nativo da AWS

**Componentes relacionados**: 
- Deployment: `ebs-csi-controller` (2 rÃ©plicas)
- DaemonSet: `ebs-csi-node` (1 por node)

**Exemplo de uso**:
```yaml
# PersistentVolumeClaim para MongoDB
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-data
spec:
  storageClassName: gp3  # â† EBS CSI provisiona automaticamente
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 20Gi
```

**Arquivo de exemplo**: `02-eks-cluster/samples/csi-sample-deployment.yml`

---

#### 3. **Metrics Server**
```
Categoria: Monitoring
FunÃ§Ã£o: Coleta mÃ©tricas de recursos (CPU/MemÃ³ria)
```

**O que faz**:
- Coleta mÃ©tricas de nodes e pods
- ExpÃµe API para `kubectl top` e HPA
- Armazena dados em memÃ³ria (curto prazo, Ãºltimos minutos)

**Por quÃª Ã© necessÃ¡rio**:
- Visualizar consumo: `kubectl top nodes` / `kubectl top pods`
- Horizontal Pod Autoscaling (HPA): escala pods baseado em CPU/memÃ³ria
- Debugging: identificar pods consumindo muitos recursos

**Componente relacionado**: Deployment `metrics-server` (2 rÃ©plicas)

**Comandos Ãºteis**:
```bash
# Ver consumo dos nodes
kubectl top nodes

# Ver consumo dos pods
kubectl top pods -A

# Ver consumo de um namespace especÃ­fico
kubectl top pods -n kube-system
```

**Arquivo Terraform**: `02-eks-cluster/eks.cluster.addons.metrics-server.tf`

---

### ComparaÃ§Ã£o: Add-ons vs InstalaÃ§Ã£o Manual

| MÃ©todo | Vantagem | Desvantagem |
|--------|----------|-------------|
| **EKS Add-ons** | Gerenciado pela AWS, atualizaÃ§Ãµes automÃ¡ticas | Menos flexibilidade |
| **Helm Charts** | Mais controle, customizÃ¡vel | VocÃª gerencia updates |
| **Manifests YAML** | Controle total | Trabalhoso de manter |

**Neste projeto**: Usamos EKS Add-ons (managed) via Terraform para facilitar manutenÃ§Ã£o.

---

## ğŸ—ï¸ ARQUITETURA COMPLETA DO PROJETO

### Diagrama de Camadas:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 0: Backend (S3 + DynamoDB)                               â”‚
â”‚  FunÃ§Ã£o: Armazena Terraform state de forma remota e segura      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: Networking (VPC + Subnets + NAT Gateways)            â”‚
â”‚  FunÃ§Ã£o: Cria rede isolada com 3 AZs                            â”‚
â”‚  â”œâ”€â”€ Public Subnets (us-east-1a, 1b, 1c)                        â”‚
â”‚  â””â”€â”€ Private Subnets (us-east-1a, 1b, 1c) â† Nodes ficam aqui   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: EKS Cluster                                           â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Control Plane (gerenciado pela AWS)                       â”‚ â”‚
â”‚  â”‚ â”œâ”€â”€ API Server                                            â”‚ â”‚
â”‚  â”‚ â”œâ”€â”€ Scheduler                                             â”‚ â”‚
â”‚  â”‚ â””â”€â”€ Controller Manager                                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Worker Nodes (3 EC2 instances)                            â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  Node 1 (us-east-1a)   Node 2 (us-east-1b)   Node 3 (1c) â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ aws-node          â”œâ”€â”€ aws-node          â”œâ”€â”€ aws-node â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ kube-proxy        â”œâ”€â”€ kube-proxy        â”œâ”€â”€ kube-pro â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ ebs-csi-node      â”œâ”€â”€ ebs-csi-node      â”œâ”€â”€ ebs-csi- â”‚ â”‚
â”‚  â”‚  â””â”€â”€ [app pods]        â””â”€â”€ [app pods]        â””â”€â”€ [app pod â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ System Pods (kube-system namespace)                       â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  Deployments (2 rÃ©plicas cada):                          â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ aws-load-balancer-controller                        â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ coredns                                             â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ ebs-csi-controller                                  â”‚ â”‚
â”‚  â”‚  â””â”€â”€ metrics-server                                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: Applications (nÃ£o criado ainda)                       â”‚
â”‚  FunÃ§Ã£o: Deploy do e-commerce (06-ecommerce-app)               â”‚
â”‚  â”œâ”€â”€ Frontend (ecommerce-ui)                                    â”‚
â”‚  â”œâ”€â”€ Backend Services (product-catalog, order-management, etc)  â”‚
â”‚  â””â”€â”€ Ingress â†’ ALB (criado pelo aws-load-balancer-controller)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” FLUXOS DE FUNCIONAMENTO

### 1. Fluxo de Networking (Request HTTP chega no cluster)

```
Internet
   â†“
Route 53 (DNS: exemplo.com â†’ ALB IP)
   â†“
Application Load Balancer (criado pelo Ingress)
   â†“
Target Group (apontando para nodes)
   â†“
kube-proxy (no node) â†’ roteia para pod correto
   â†“
Pod (container da aplicaÃ§Ã£o)
```

### 2. Fluxo de Storage (Pod precisa de volume persistente)

```
Desenvolvedor â†’ cria PersistentVolumeClaim (PVC)
   â†“
ebs-csi-controller â†’ cria volume EBS na AWS
   â†“
ebs-csi-node â†’ attach o volume no node
   â†“
Kubelet â†’ monta o volume no pod
   â†“
Pod â†’ acessa o disco em /data
```

### 3. Fluxo de ComunicaÃ§Ã£o entre Pods

```
Pod A (frontend) quer chamar Pod B (backend)
   â†“
Frontend chama: http://product-catalog:8080
   â†“
CoreDNS â†’ resolve "product-catalog" â†’ retorna IP do Service
   â†“
kube-proxy â†’ roteia para um dos pods do backend (load balance)
   â†“
Pod B (backend) â†’ processa request â†’ retorna resposta
```

---

## ğŸ“Š CONTADORES E MATEMÃTICA

### Por que 17 pods no total?

```
Deployments (2 rÃ©plicas cada):
  2 Ã— aws-load-balancer-controller
  2 Ã— coredns
  2 Ã— ebs-csi-controller
  2 Ã— metrics-server
  = 8 pods

DaemonSets (1 por node Ã— 3 nodes):
  3 Ã— aws-node
  3 Ã— kube-proxy
  3 Ã— ebs-csi-node
  = 9 pods

TOTAL: 8 + 9 = 17 pods âœ“
```

### E se escalarmos para 5 nodes?

```
Deployments: 8 pods (nÃ£o muda, sÃ£o 2 rÃ©plicas fixas)
DaemonSets: 5 Ã— 3 = 15 pods (1 de cada tipo por node)

TOTAL: 8 + 15 = 23 pods
```

---

## ğŸ“ CONCEITOS EXTRAS IMPORTANTES

### 1. **Namespaces** (Isolamento LÃ³gico)

**O que sÃ£o**: "Pastas virtuais" dentro do cluster.

**No projeto**:
```bash
# Pods de sistema
kube-system â†’ aws-load-balancer-controller, coredns, etc

# AplicaÃ§Ãµes (serÃ¡ criado depois)
default â†’ onde vocÃª coloca suas apps
ecommerce â†’ namespace customizado para o e-commerce
```

**Por quÃª usar**:
- OrganizaÃ§Ã£o: separar ambientes (dev, staging, prod)
- SeguranÃ§a: RBAC por namespace
- Resource Quotas: limitar CPU/memÃ³ria por namespace

---

### 2. **Services** (AbstraÃ§Ã£o de Rede)

**O que sÃ£o**: IP fixo e DNS name para acessar pods (que tÃªm IPs dinÃ¢micos).

**Tipos**:
```yaml
# ClusterIP (padrÃ£o): acessÃ­vel apenas dentro do cluster
kind: Service
type: ClusterIP

# NodePort: expÃµe porta em todos os nodes
type: NodePort

# LoadBalancer: cria ALB/NLB na AWS
type: LoadBalancer
```

**Exemplo do projeto** (serÃ¡ criado no deploy):
```yaml
# 06-ecommerce-app/manifests/product-catalog.yaml
apiVersion: v1
kind: Service
metadata:
  name: product-catalog
spec:
  type: ClusterIP
  selector:
    app: product-catalog
  ports:
  - port: 8080
    targetPort: 8080
```

---

### 3. **Ingress** (Roteamento HTTP/HTTPS)

**O que Ã©**: Regras de roteamento para expor mÃºltiplos serviÃ§os via 1 ALB.

**Sem Ingress**:
- Cada serviÃ§o = 1 LoadBalancer = 1 ALB = $$$ (caro!)

**Com Ingress**:
- 1 ALB para todos os serviÃ§os
- Roteamento por path: `/api` â†’ backend, `/` â†’ frontend

**Exemplo do projeto** (`06-ecommerce-app/manifests/ingress.yaml`):
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ecommerce-ingress
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
spec:
  rules:
  - host: ecommerce.exemplo.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ecommerce-ui
            port: 80
```

**Como funciona**:
1. VocÃª cria o Ingress
2. aws-load-balancer-controller detecta
3. Controller cria ALB automaticamente na AWS
4. ALB roteia para Services â†’ Pods

---

### 4. **OIDC (OpenID Connect)** - AutenticaÃ§Ã£o para Pods

**O que Ã©**: Permite pods assumirem IAM Roles sem precisar de access keys.

**Problema sem OIDC**:
```python
# Pod precisa acessar S3
# Ruim: hardcodear credentials
aws_access_key = "AKIAIOSFODNN7EXAMPLE"
aws_secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
```

**SoluÃ§Ã£o com OIDC**:
```yaml
# Associar ServiceAccount com IAM Role
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456:role/pod-s3-role
```

**Usado no projeto**: 
- ALB Controller precisa criar ALBs â†’ usa OIDC + IAM Role
- External DNS precisa modificar Route53 â†’ usa OIDC + IAM Role

**Arquivo**: `02-eks-cluster/eks.cluster.oidc.tf`

---

### 5. **External DNS** (AutomaÃ§Ã£o de DNS)

**O que faz**: Cria/atualiza registros DNS no Route53 automaticamente.

**Fluxo**:
1. VocÃª cria Ingress com `host: api.exemplo.com`
2. External DNS detecta a anotaÃ§Ã£o
3. Cria registro no Route53: `api.exemplo.com â†’ ALB DNS`
4. UsuÃ¡rio acessa `api.exemplo.com` â†’ Route53 â†’ ALB â†’ Pods

**Arquivo**: `02-eks-cluster/eks.cluster.external.dns.tf`

---

## ğŸ” SEGURANÃ‡A E BOAS PRÃTICAS

### 1. **RBAC (Role-Based Access Control)**

```yaml
# Exemplo: dar permissÃ£o para um usuÃ¡rio apenas ler pods
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
```

**No projeto**: Configurado via `02-eks-cluster/eks.cluster.access.tf`

---

### 2. **Secrets** (Credenciais Seguras)

```bash
# Criar secret
kubectl create secret generic db-password \
  --from-literal=password=supersecret

# Usar no pod
env:
- name: DB_PASSWORD
  valueFrom:
    secretKeyRef:
      name: db-password
      key: password
```

---

### 3. **Resource Limits** (Prevenir um pod consumir tudo)

```yaml
resources:
  requests:    # MÃ­nimo garantido
    cpu: 100m
    memory: 128Mi
  limits:      # MÃ¡ximo permitido
    cpu: 500m
    memory: 512Mi
```

---

## ğŸ“ COMANDOS ÃšTEIS PARA AULAS

### ExploraÃ§Ã£o BÃ¡sica

```bash
# Listar todos os recursos
kubectl get all -A

# Ver detalhes de um pod
kubectl describe pod <pod-name> -n kube-system

# Ver logs de um pod
kubectl logs <pod-name> -n kube-system

# Entrar em um pod (debug)
kubectl exec -it <pod-name> -n kube-system -- /bin/bash

# Ver eventos do cluster
kubectl get events -A --sort-by='.lastTimestamp'
```

### Monitoramento

```bash
# Ver consumo de recursos
kubectl top nodes
kubectl top pods -A

# Ver capacidade do cluster
kubectl describe nodes | grep -A 5 "Allocated resources"

# Ver quais pods estÃ£o em qual node
kubectl get pods -A -o wide
```

### Debugging

```bash
# Por que um pod nÃ£o estÃ¡ rodando?
kubectl describe pod <pod-name> -n <namespace>

# Ver logs com follow
kubectl logs -f <pod-name> -n <namespace>

# Ver logs anteriores (se pod crashou)
kubectl logs <pod-name> --previous -n <namespace>

# Port-forward para testar serviÃ§o localmente
kubectl port-forward svc/<service-name> 8080:80 -n <namespace>
```

### InspeÃ§Ã£o de ConfiguraÃ§Ãµes

```bash
# Ver o YAML completo de um recurso
kubectl get deployment aws-load-balancer-controller -n kube-system -o yaml

# Ver apenas as labels
kubectl get pods -n kube-system --show-labels

# Filtrar por label
kubectl get pods -l app=coredns -n kube-system

# Ver todos os Services
kubectl get svc -A

# Ver todos os Ingress
kubectl get ingress -A
```

---

## ğŸ¯ EXERCÃCIOS PARA ALUNOS

### NÃ­vel 1: ObservaÃ§Ã£o

1. Liste todos os nodes e identifique em qual subnet cada um estÃ¡
2. Conte quantos pods de cada DaemonSet existem e explique o nÃºmero
3. Identifique qual pod consome mais CPU/memÃ³ria

### NÃ­vel 2: InvestigaÃ§Ã£o

4. Descubra qual IAM Role o aws-load-balancer-controller estÃ¡ usando
5. Encontre quais portas o CoreDNS estÃ¡ escutando
6. Identifique qual imagem Docker cada pod estÃ¡ usando

### NÃ­vel 3: ExperimentaÃ§Ã£o

7. Delete um pod do coredns e observe o que acontece (ReplicaSet recria)
8. FaÃ§a port-forward de um serviÃ§o e teste localmente
9. Tente criar um pod simples (nginx) e veja em qual node ele foi alocado

### NÃ­vel 4: Troubleshooting

10. Simule um problema: escale coredns para 0 rÃ©plicas e teste resoluÃ§Ã£o DNS
11. Remova o aws-load-balancer-controller e tente criar um Ingress
12. Analise os logs do metrics-server para entender como coleta dados

---

## ğŸ“š REFERÃŠNCIAS E MATERIAIS EXTRAS

### DocumentaÃ§Ã£o Oficial

- **Kubernetes**: https://kubernetes.io/docs/
- **Amazon EKS**: https://docs.aws.amazon.com/eks/
- **AWS Load Balancer Controller**: https://kubernetes-sigs.github.io/aws-load-balancer-controller/
- **EBS CSI Driver**: https://github.com/kubernetes-sigs/aws-ebs-csi-driver

### Arquivos do Projeto Relacionados

| Conceito | Arquivo |
|----------|---------|
| ConfiguraÃ§Ã£o do Cluster | [02-eks-cluster/eks.cluster.tf](../02-eks-cluster/eks.cluster.tf) |
| Node Groups | [02-eks-cluster/eks.cluster.node-group.tf](../02-eks-cluster/eks.cluster.node-group.tf) |
| Add-ons (CSI) | [02-eks-cluster/eks.cluster.addons.csi.tf](../02-eks-cluster/eks.cluster.addons.csi.tf) |
| Metrics Server | [02-eks-cluster/eks.cluster.addons.metrics-server.tf](../02-eks-cluster/eks.cluster.addons.metrics-server.tf) |
| ALB Controller | [02-eks-cluster/eks.cluster.external.alb.tf](../02-eks-cluster/eks.cluster.external.alb.tf) |
| External DNS | [02-eks-cluster/eks.cluster.external.dns.tf](../02-eks-cluster/eks.cluster.external.dns.tf) |
| OIDC | [02-eks-cluster/eks.cluster.oidc.tf](../02-eks-cluster/eks.cluster.oidc.tf) |
| Subnets | [01-networking/vpc.private-subnets.tf](../01-networking/vpc.private-subnets.tf) |

---

## ğŸ¬ CONCLUSÃƒO

Este cluster EKS foi construÃ­do seguindo **AWS Well-Architected Framework**:

âœ… **Confiabilidade**: 3 nodes em 3 AZs, rÃ©plicas de pods crÃ­ticos  
âœ… **SeguranÃ§a**: OIDC, RBAC, pods em private subnets  
âœ… **EficiÃªncia de Performance**: Metrics Server, HPA, resource limits  
âœ… **OtimizaÃ§Ã£o de Custos**: Add-ons managed (menos overhead operacional)  
âœ… **ExcelÃªncia Operacional**: IaC com Terraform, GitOps ready  

**PrÃ³ximos Passos**: Deploy das aplicaÃ§Ãµes e observaÃ§Ã£o dos Ingress/ALB em aÃ§Ã£o!

---

*Documento criado para fins educacionais - GitOps EKS Project*  
*Ãšltima atualizaÃ§Ã£o: Janeiro 2026*
